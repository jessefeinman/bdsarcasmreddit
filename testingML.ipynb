{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ml import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process 50 tweets, if n is ommitted it processes all of them, sets lable and saves the processed tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288.385695\n"
     ]
    }
   ],
   "source": [
    "sarcasticFeats = processTweets(jsonFileName=JSON_DIR+\"sarcastic/unique.json\", sarcastic=True, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290.641063\n"
     ]
    }
   ],
   "source": [
    "seriousFeats = processTweets(jsonFileName=JSON_DIR+\"non_sarcastic/unique.json\", sarcastic=False, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the processed tweets (if you previously processed them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sarcasticFeats = loadFeatures(sarcastic=True)\n",
    "seriousFeats = loadFeatures(sarcastic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize the ordering and return a list which has equal sarcastic/non-sarcastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(features, sarcasm) = shuffleFeatures(sarcasticFeats, seriousFeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten feature dictionaries, if leaveout is a feature that feature is ommitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = flattenFeatureDicts(features, leaveOut=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize and fit, then save the features and the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(dv, X, y) = vectorize(features, sarcasm, fit=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load vectorizer and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(dv, X, y) = loadVectorizer(features=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If using a loaded vectorizer, with unprocessed features you have to transform the features into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X, y) = vectorizerTransform(dv, features, sarcasm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test, reports results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 0: 2017-04-13 00:17:40.671672\n",
      "\n",
      "\n",
      "Features before reduction: (70944, 1616835)\n",
      "\n",
      "\n",
      "Features after reduction: (70944, 1616)\n",
      "\n",
      "\n",
      "Starting to train...\n",
      "Trained:\t0\t<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 29\n",
      "\n",
      "\n",
      "Starting to test...\n",
      "Tested:\t0\t<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 0\tScore:\t0.909901\n",
      "Iteration time:\t43\n",
      "Total elapsed time:\t43\n",
      "Starting iteration 1: 2017-04-13 00:18:24.557311\n",
      "\n",
      "\n",
      "Features before reduction: (70944, 1616835)\n",
      "\n",
      "\n",
      "Features after reduction: (70944, 1616)\n",
      "\n",
      "\n",
      "Starting to train...\n",
      "Trained:\t0\t<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 21\n",
      "\n",
      "\n",
      "Starting to test...\n",
      "Tested:\t0\t<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 0\tScore:\t0.907307\n",
      "Iteration time:\t35\n",
      "Total elapsed time:\t79\n",
      "Starting iteration 2: 2017-04-13 00:19:00.268183\n",
      "\n",
      "\n",
      "Features before reduction: (70944, 1616835)\n",
      "\n",
      "\n",
      "Features after reduction: (70944, 1617)\n",
      "\n",
      "\n",
      "Starting to train...\n",
      "Trained:\t0\t<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 22\n",
      "\n",
      "\n",
      "Starting to test...\n",
      "Tested:\t0\t<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 0\tScore:\t0.911479\n",
      "Iteration time:\t35\n",
      "Total elapsed time:\t115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(0,\n",
       "  0,\n",
       "  \"<class 'sklearn.linear_model.logistic.LogisticRegression'>\"): {'score': 0.90990076680198462, 'testTime': 0.003005},\n",
       " (1,\n",
       "  0,\n",
       "  \"<class 'sklearn.linear_model.logistic.LogisticRegression'>\"): {'score': 0.90730717185385656, 'testTime': 0.003001},\n",
       " (2,\n",
       "  0,\n",
       "  \"<class 'sklearn.linear_model.logistic.LogisticRegression'>\"): {'score': 0.91147947677041041, 'testTime': 0.002999}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainTest(X, y, dv, reduce=0.1, splits=3, trainsize=0.8, classifiers=DEFAULT_CLASSIFIERS, voting=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 0: 2017-04-12 23:23:45.832261\n",
      "\n",
      "\n",
      "Starting to train & Test...\n",
      "Iteration time:\t0\n",
      "Total elapsed time:\t0\n",
      "Starting iteration 1: 2017-04-12 23:23:46.340261\n",
      "\n",
      "\n",
      "Starting to train & Test...\n",
      "Iteration time:\t0\n",
      "Total elapsed time:\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(X, y, dv, reduce=0, splits=2, trainsize=0.5, classifiers=DEFAULT_CLASSIFIERS_ARGS, crossValidation=10, maxIter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, \"<class 'sklearn.linear_model.logistic.LogisticRegression'>\"): {'prediction': array([False, False], dtype=bool), 'prediction_probabilities': array([[  9.99340758e-01,   6.59241560e-04],\n",
      "       [  9.99774456e-01,   2.25543687e-04]])}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(0,\n",
       "  \"<class 'sklearn.linear_model.logistic.LogisticRegression'>\"): {'prediction': array([False, False], dtype=bool), 'prediction_probabilities': array([[  9.99340758e-01,   6.59241560e-04],\n",
       "         [  9.99774456e-01,   2.25543687e-04]])}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfClassifiersDV = loadClassifiersDV()\n",
    "predict([\"some dumb text\",\"some more of it\"], listOfClassifiersDV[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most imformative features of a saved classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!_RAW',\n",
       " '\"_RAW',\n",
       " '#_RAW',\n",
       " '#_RAW/LEN',\n",
       " '#_RAW/TOTAL_PUNCT_FOUND',\n",
       " \"'_RAW/LEN\",\n",
       " \"'_RAW/TOTAL_PUNCT_FOUND\",\n",
       " ',_RAW',\n",
       " '._RAW',\n",
       " '._RAW/TOTAL_PUNCT_FOUND',\n",
       " '/_RAW',\n",
       " '/_RAW/LEN',\n",
       " '/_RAW/TOTAL_PUNCT_FOUND',\n",
       " ':_RAW',\n",
       " ':_RAW/LEN',\n",
       " ':_RAW/TOTAL_PUNCT_FOUND',\n",
       " '?_RAW',\n",
       " '@_RAW/LEN',\n",
       " 'TOTAL',\n",
       " '__RAW/LEN',\n",
       " '__RAW/TOTAL_PUNCT_FOUND',\n",
       " 'grm1 !',\n",
       " \"grm1 's\",\n",
       " 'grm1 ,',\n",
       " 'grm1 -',\n",
       " 'grm1 .',\n",
       " 'grm1 ...',\n",
       " 'grm1 ?',\n",
       " 'grm1 RT',\n",
       " 'grm1 \\\\n',\n",
       " 'grm1 \\\\n\\\\n',\n",
       " 'grm1 ``',\n",
       " 'grm1 a',\n",
       " 'grm1 all',\n",
       " 'grm1 are',\n",
       " 'grm1 is',\n",
       " \"grm1 n't\",\n",
       " 'grm1 of',\n",
       " 'grm1 sure',\n",
       " 'grm1 that',\n",
       " 'grm1 the',\n",
       " 'grm1 â€¦',\n",
       " 'grm2 ! ``',\n",
       " \"grm2 ' RT\",\n",
       " 'grm2 - NameTOK',\n",
       " \"grm2 . '\",\n",
       " \"grm2 . ''\",\n",
       " 'grm2 . NameTOK',\n",
       " 'grm2 ? ``',\n",
       " 'grm2 Hey ,',\n",
       " \"grm2 NameTOK '\",\n",
       " \"grm2 \\\\n '\",\n",
       " 'grm2 `` NameTOK',\n",
       " 'grm2 â€¦ NameTOK',\n",
       " \"grm3 . NameTOK '\",\n",
       " \"grm3 NameTOK NameTOK '\",\n",
       " 'grm3 `` NameTOK NameTOK',\n",
       " 'pos1 ,',\n",
       " 'pos1 .',\n",
       " 'pos1 DT',\n",
       " 'pos1 IN',\n",
       " 'pos1 JJ',\n",
       " 'pos1 MD',\n",
       " 'pos1 NN',\n",
       " 'pos1 NNS',\n",
       " 'pos1 POS',\n",
       " 'pos1 PRP',\n",
       " 'pos1 RB',\n",
       " 'pos1 UH',\n",
       " 'pos1 VB',\n",
       " 'pos1 VBG',\n",
       " 'pos1 VBP',\n",
       " 'pos1 VBZ',\n",
       " 'pos1 WP',\n",
       " 'pos1 WRB',\n",
       " 'pos1 ``',\n",
       " 'pos2 . NNP',\n",
       " 'pos2 . PRP',\n",
       " 'pos2 . [NE]',\n",
       " 'pos2 . ``',\n",
       " 'pos2 : [NE]',\n",
       " 'pos2 CD [NE]',\n",
       " 'pos2 DT JJ',\n",
       " 'pos2 DT NN',\n",
       " 'pos2 DT VBZ',\n",
       " 'pos2 IN DT',\n",
       " 'pos2 JJ .',\n",
       " 'pos2 JJ ``',\n",
       " 'pos2 NN .',\n",
       " 'pos2 NN ``',\n",
       " 'pos2 NNP NNP',\n",
       " 'pos2 NNS .',\n",
       " 'pos2 PRP VBP',\n",
       " 'pos2 RB .',\n",
       " 'pos2 RB JJ',\n",
       " 'pos2 RB VB',\n",
       " 'pos2 UH ,',\n",
       " 'pos2 VB DT',\n",
       " 'pos2 VBP ``',\n",
       " 'pos2 VBZ DT',\n",
       " 'pos2 VBZ RB',\n",
       " 'pos2 [NE] POS',\n",
       " 'pos2 `` UH',\n",
       " 'pos3 , RB .',\n",
       " 'pos3 . NNP ,',\n",
       " 'pos3 . [NE] POS',\n",
       " 'pos3 DT NN ``',\n",
       " 'pos3 IN [NE] POS',\n",
       " 'pos3 JJ NN ``',\n",
       " 'pos3 PRP VBP ``',\n",
       " 'pos3 [NE] NNP NNP',\n",
       " 'pos3 `` UH ,',\n",
       " 'suf1 ing',\n",
       " 'suf1 ly',\n",
       " 'suf1 s',\n",
       " 'suf2 s ly',\n",
       " 'suf2 s s',\n",
       " 'syl1 1',\n",
       " 'syl1 2',\n",
       " 'syl1 3',\n",
       " 'syl1 4',\n",
       " 'syl2 1 1',\n",
       " 'syl2 1 2',\n",
       " 'syl2 1 3',\n",
       " 'syl2 1 4',\n",
       " 'syl2 2 1',\n",
       " 'syl2 3 1',\n",
       " 'syl2 4 1',\n",
       " 'syl3 1 1 1',\n",
       " 'syl3 1 1 2',\n",
       " 'syl3 1 1 3',\n",
       " 'syl3 1 1 4',\n",
       " 'syl3 1 2 1',\n",
       " 'syl3 1 3 1',\n",
       " 'syl3 1 4 1',\n",
       " 'syl3 2 1 1',\n",
       " 'syl3 3 1 1',\n",
       " 'syl4 1 1 1 1',\n",
       " 'syl4 1 1 1 2',\n",
       " 'syl4 1 1 1 3',\n",
       " 'syl4 1 1 2 1',\n",
       " 'syl4 1 1 3 1',\n",
       " 'syl4 1 2 1 1',\n",
       " 'syl4 1 3 1 1',\n",
       " 'syl4 2 1 1 1',\n",
       " 'vow1 0',\n",
       " 'vow1 1',\n",
       " 'vow2 0 1',\n",
       " 'vow2 1 0',\n",
       " 'vow2 1 1',\n",
       " 'vow3 0 1 0',\n",
       " 'vow3 0 1 1',\n",
       " 'vow3 1 0 1',\n",
       " 'vow3 1 1 0',\n",
       " 'vow3 1 1 1',\n",
       " 'vow4 0 1 0 1',\n",
       " 'vow4 0 1 1 1',\n",
       " 'vow4 1 0 1 0',\n",
       " 'vow4 1 0 1 1',\n",
       " 'vow4 1 1 0 1',\n",
       " 'vow4 1 1 1 0',\n",
       " 'vow4 1 1 1 1']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfClassifiersDV = loadClassifiersDV('pickled/2017-04-13000301013618 0.pickle')\n",
    "dvc = listOfClassifiersDV[0][1]\n",
    "dvc.get_feature_names()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
