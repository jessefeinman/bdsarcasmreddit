{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ml\n",
    "import nlp\n",
    "import json_io\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from dvs import DictVectorizerPartial\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = ml.JSON_DIR+\"reddit/\"\n",
    "sarcastic_path = path+\"sarcastic/\"\n",
    "serious_path = path+\"serious/\"\n",
    "source = '-reddit-'\n",
    "features_path = 'features/'\n",
    "n=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_io.processRandomizeJson(sarcastic=True,\n",
    "                     json_path=sarcastic_path,\n",
    "                     features_path=features_path,\n",
    "                     source=source,\n",
    "                     n=n,\n",
    "                     cleanTokens=nlp.cleanTokensReddit)\n",
    "json_io.processRandomizeJson(sarcastic=False,\n",
    "                     json_path=serious_path,\n",
    "                     features_path=features_path,\n",
    "                     source=source,\n",
    "                     n=n,\n",
    "                     cleanTokens=nlp.cleanTokensReddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sarcasticFeats = json_io.loadProcessedFeatures(features_path,\n",
    "                                       source,\n",
    "                                       sarcastic=True,\n",
    "                                       n=5,\n",
    "                                       random=False)\n",
    "seriousFeats = json_io.loadProcessedFeatures(features_path,\n",
    "                                     source,\n",
    "                                     sarcastic=False,\n",
    "                                     n=3,\n",
    "                                     random=False)\n",
    "features = chain(sarcasticFeats, seriousFeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten feature dictionaries, if leaveout is a feature that feature is ommitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dvp = DictVectorizerPartial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X,y) = ml.split_feat(features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X,y) = ml.flatten(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X,y) = (dvp.partial_fit_transform(X), np.array(list(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(dvp, open('pickled/-reddit-dvp.pickle', 'wb'))\n",
    "pickle.dump(y, open('pickled/-reddit-y.pickle', 'wb'))\n",
    "pickle.dump(X, open('pickled/-reddit-X.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pickle.load(open('pickled/-reddit-X.pickle', 'rb'))\n",
    "y = pickle.load(open('pickled/-reddit-y.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test, reports results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tReduction: 0\n",
      "\n",
      "\t\tTraining size: 0.01\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.568462\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.619744\n",
      "\n",
      "\t\tTraining size: 0.05\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.673811\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 1\tScore:\t0.691526\n",
      "\n",
      "\t\tTraining size: 0.1\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 1\tScore:\t0.700810\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 1\tScore:\t0.702373\n",
      "\n",
      "\t\tTraining size: 0.2\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 1\tScore:\t0.715615\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 1\tScore:\t0.714991\n",
      "\n",
      "\t\tTraining size: 0.4\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 2\tScore:\t0.725965\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 2\tScore:\t0.727018\n",
      "\n",
      "\t\tTraining size: 0.6\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 2\tScore:\t0.730205\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 3\tScore:\t0.732338\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 3\tScore:\t0.734310\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 3\tScore:\t0.733505\n",
      "\n",
      "\t\tReduction: 2500000\n",
      "\n",
      "\t\tTraining size: 0.01\n",
      "Features before reduction: (7453, 12490143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [0 0 0 ..., 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after reduction: (7453, 2500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.639258\n",
      "Features before reduction: (7453, 12490143)\n",
      "Features after reduction: (7453, 2500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.640526\n",
      "\n",
      "\t\tTraining size: 0.05\n",
      "Features before reduction: (37268, 12490143)\n",
      "Features after reduction: (37268, 2500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.695404\n",
      "Features before reduction: (37268, 12490143)\n",
      "Features after reduction: (37268, 2500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.696115\n",
      "\n",
      "\t\tTraining size: 0.1\n",
      "Features before reduction: (74537, 12490143)\n",
      "Features after reduction: (74537, 2500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.708484\n",
      "Features before reduction: (74537, 12490143)\n",
      "Features after reduction: (74537, 2500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.706619\n",
      "\n",
      "\t\tTraining size: 0.2\n",
      "Features before reduction: (149075, 12490143)\n",
      "Features after reduction: (149075, 2500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.685355\n",
      "Features before reduction: (149075, 12490143)\n",
      "Features after reduction: (149075, 2500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.683658\n",
      "\n",
      "\t\tTraining size: 0.4\n",
      "Features before reduction: (298150, 12490143)\n",
      "Features after reduction: (298150, 2500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 1\tScore:\t0.655384\n",
      "Features before reduction: (298150, 12490143)\n",
      "Features after reduction: (298150, 2500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 1\tScore:\t0.655250\n",
      "\n",
      "\t\tTraining size: 0.6\n",
      "Features before reduction: (447226, 12490143)\n",
      "Features after reduction: (447226, 2500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 1\tScore:\t0.701991\n",
      "Features before reduction: (447226, 12490143)\n",
      "Features after reduction: (447226, 2500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 1\tScore:\t0.700321\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Features before reduction: (596301, 12490143)\n",
      "Features after reduction: (596301, 2500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 1\tScore:\t0.720042\n",
      "Features before reduction: (596301, 12490143)\n",
      "Features after reduction: (596301, 2500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 1\tScore:\t0.720961\n",
      "\n",
      "\t\tReduction: 1000000\n",
      "\n",
      "\t\tTraining size: 0.01\n",
      "Features before reduction: (7453, 12490143)\n",
      "Features after reduction: (7453, 1000000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.662112\n",
      "Features before reduction: (7453, 12490143)\n",
      "Features after reduction: (7453, 1000000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.650440\n",
      "\n",
      "\t\tTraining size: 0.05\n",
      "Features before reduction: (37268, 12490143)\n",
      "Features after reduction: (37268, 1000000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.693438\n",
      "Features before reduction: (37268, 12490143)\n",
      "Features after reduction: (37268, 1000000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.696611\n",
      "\n",
      "\t\tTraining size: 0.1\n",
      "Features before reduction: (74537, 12490143)\n",
      "Features after reduction: (74537, 1000000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.569112\n",
      "Features before reduction: (74537, 12490143)\n",
      "Features after reduction: (74537, 1000000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.568321\n",
      "\n",
      "\t\tTraining size: 0.2\n",
      "Features before reduction: (149075, 12490143)\n",
      "Features after reduction: (149075, 1000000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.671322\n",
      "Features before reduction: (149075, 12490143)\n",
      "Features after reduction: (149075, 1000000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.671020\n",
      "\n",
      "\t\tTraining size: 0.4\n",
      "Features before reduction: (298150, 12490143)\n",
      "Features after reduction: (298150, 1000000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.719666\n",
      "Features before reduction: (298150, 12490143)\n",
      "Features after reduction: (298150, 1000000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.720840\n",
      "\n",
      "\t\tTraining size: 0.6\n",
      "Features before reduction: (447226, 12490143)\n",
      "Features after reduction: (447226, 1000000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 1\tScore:\t0.728608\n",
      "Features before reduction: (447226, 12490143)\n",
      "Features after reduction: (447226, 1000000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 1\tScore:\t0.729869\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Features before reduction: (596301, 12490143)\n",
      "Features after reduction: (596301, 1000000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 1\tScore:\t0.733693\n",
      "Features before reduction: (596301, 12490143)\n",
      "Features after reduction: (596301, 1000000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 1\tScore:\t0.732143\n",
      "\n",
      "\t\tReduction: 500000\n",
      "\n",
      "\t\tTraining size: 0.01\n",
      "Features before reduction: (7453, 12490143)\n",
      "Features after reduction: (7453, 500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.658597\n",
      "Features before reduction: (7453, 12490143)\n",
      "Features after reduction: (7453, 500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.658885\n",
      "\n",
      "\t\tTraining size: 0.05\n",
      "Features before reduction: (37268, 12490143)\n",
      "Features after reduction: (37268, 500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.551175\n",
      "Features before reduction: (37268, 12490143)\n",
      "Features after reduction: (37268, 500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.556629\n",
      "\n",
      "\t\tTraining size: 0.1\n",
      "Features before reduction: (74537, 12490143)\n",
      "Features after reduction: (74537, 500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.662910\n",
      "Features before reduction: (74537, 12490143)\n",
      "Features after reduction: (74537, 500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.661837\n",
      "\n",
      "\t\tTraining size: 0.2\n",
      "Features before reduction: (149075, 12490143)\n",
      "Features after reduction: (149075, 500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.715749\n",
      "Features before reduction: (149075, 12490143)\n",
      "Features after reduction: (149075, 500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.715306\n",
      "\n",
      "\t\tTraining size: 0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features before reduction: (298150, 12490143)\n",
      "Features after reduction: (298150, 500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.722906\n",
      "Features before reduction: (298150, 12490143)\n",
      "Features after reduction: (298150, 500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.725992\n",
      "\n",
      "\t\tTraining size: 0.6\n",
      "Features before reduction: (447226, 12490143)\n",
      "Features after reduction: (447226, 500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 1\tScore:\t0.726401\n",
      "Features before reduction: (447226, 12490143)\n",
      "Features after reduction: (447226, 500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 1\tScore:\t0.729252\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Features before reduction: (596301, 12490143)\n",
      "Features after reduction: (596301, 500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 1\tScore:\t0.732063\n",
      "Features before reduction: (596301, 12490143)\n",
      "Features after reduction: (596301, 500000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 1\tScore:\t0.729534\n",
      "\n",
      "\t\tReduction: 100000\n",
      "\n",
      "\t\tTraining size: 0.01\n",
      "Features before reduction: (7453, 12490143)\n",
      "Features after reduction: (7453, 100000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.536954\n",
      "Features before reduction: (7453, 12490143)\n",
      "Features after reduction: (7453, 100000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.531903\n",
      "\n",
      "\t\tTraining size: 0.05\n",
      "Features before reduction: (37268, 12490143)\n",
      "Features after reduction: (37268, 100000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.698476\n",
      "Features before reduction: (37268, 12490143)\n",
      "Features after reduction: (37268, 100000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.697812\n",
      "\n",
      "\t\tTraining size: 0.1\n",
      "Features before reduction: (74537, 12490143)\n",
      "Features after reduction: (74537, 100000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.704721\n",
      "Features before reduction: (74537, 12490143)\n",
      "Features after reduction: (74537, 100000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.706968\n",
      "\n",
      "\t\tTraining size: 0.2\n",
      "Features before reduction: (149075, 12490143)\n",
      "Features after reduction: (149075, 100000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.715903\n",
      "Features before reduction: (149075, 12490143)\n",
      "Features after reduction: (149075, 100000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.715655\n",
      "\n",
      "\t\tTraining size: 0.4\n",
      "Features before reduction: (298150, 12490143)\n",
      "Features after reduction: (298150, 100000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.722041\n",
      "Features before reduction: (298150, 12490143)\n",
      "Features after reduction: (298150, 100000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.720941\n",
      "\n",
      "\t\tTraining size: 0.6\n",
      "Features before reduction: (447226, 12490143)\n",
      "Features after reduction: (447226, 100000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.724308\n",
      "Features before reduction: (447226, 12490143)\n",
      "Features after reduction: (447226, 100000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.724476\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Features before reduction: (596301, 12490143)\n",
      "Features after reduction: (596301, 100000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.725066\n",
      "Features before reduction: (596301, 12490143)\n",
      "Features after reduction: (596301, 100000)\n",
      "Starting to train <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\tTime: 0\tScore:\t0.724436\n",
      "[(0, 0.01, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.629998, 0.56846172422120256), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.696998, 0.61974429150232091)]), (0, 0.05, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.880995, 0.6738106737503019), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 1.031438, 0.69152646972014276)]), (0, 0.1, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 1.186007, 0.70081032493493256), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 1.079014, 0.70237328610909866)]), (0, 0.2, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 1.6125, 0.71561485416834369), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 1.532959, 0.71499101129625153)]), (0, 0.4, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 2.226014, 0.72596527945477474), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 2.223394, 0.72701843355067219)]), (0, 0.6, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 2.945503, 0.73020472778985213), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 3.008574, 0.73233786793313482)]), (0, 0.8, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 3.837503, 0.73431001636749038), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 3.767503, 0.73350505782285547)]), (2500000, 0.01, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.210503, 0.63925782822184662, array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.169001, 0.64052563792964667, array([ True,  True,  True, ...,  True,  True,  True], dtype=bool))]), (2500000, 0.05, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.345001, 0.69540368671013442, array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.289004, 0.69611473342456198, array([ True,  True,  True, ...,  True,  True,  True], dtype=bool))]), (2500000, 0.1, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.438005, 0.70848426306045242, array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.439999, 0.70661944243204811, array([ True,  True,  True, ...,  True,  True,  True], dtype=bool))]), (2500000, 0.2, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.569001, 0.68535512087794148, array([ True,  True,  True, ..., False, False, False], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.593003, 0.68365799994633614, array([ True,  True,  True, ..., False, False, False], dtype=bool))]), (2500000, 0.4, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 1.028001, 0.65538383106603348, array([ True,  True,  True, ..., False, False, False], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 1.024999, 0.65524967130859424, array([ True,  True,  True, ..., False, False, False], dtype=bool))]), (2500000, 0.6, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 1.506999, 0.70199093080039709, array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 1.512, 0.70032064182027964, array([ True,  True,  True, ...,  True,  True,  True], dtype=bool))]), (2500000, 0.8, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 1.965998, 0.72004212616383589, array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 1.924, 0.72096112050229411, array([ True,  True,  True, ...,  True,  True,  True], dtype=bool))]), (1000000, 0.01, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.083008, 0.66211194290160724, array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.078998, 0.65044004400440047, array([ True,  True,  True, ...,  True,  True,  True], dtype=bool))]), (1000000, 0.05, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.163, 0.6934382462636508, array([ True,  True, False, ..., False, False, False], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.151999, 0.69661112452708684, array([ True,  True,  True, ..., False, False, False], dtype=bool))]), (1000000, 0.1, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.233001, 0.5691123990447825, array([ True,  True,  True, ..., False, False, False], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.239001, 0.56832085647589148, array([ True,  True,  True, ..., False, False, False], dtype=bool))]), (1000000, 0.2, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.411001, 0.67132201024980542, array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.418003, 0.67102015079556732, array([ True,  True,  True, ..., False, False, False], dtype=bool))]), (1000000, 0.4, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.827001, 0.71966647884300627, array([ True,  True,  True, ..., False, False, False], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.823998, 0.72084037672059886, array([ True,  True,  True, ...,  True,  True,  True], dtype=bool))]), (1000000, 0.6, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 1.244996, 0.72860822667632619, array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 1.239, 0.72986932839625429, array([ True,  True,  True, ...,  True,  True,  True], dtype=bool))]), (1000000, 0.8, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 1.685997, 0.73369288148327028, array([ True,  True,  True, ..., False, False, False], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 1.598995, 0.73214333628484796, array([ True,  True,  True, ..., False, False, False], dtype=bool))]), (500000, 0.01, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.055975, 0.65859695725670131, array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.055999, 0.65888540073519553, array([ True,  True,  True, ...,  True,  True,  True], dtype=bool))]), (500000, 0.05, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.106999, 0.551175239475167, array([ True,  True,  True, ..., False, False, False], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.108001, 0.55662883361506887, array([ True, False, False, ..., False, False, False], dtype=bool))]), (500000, 0.1, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.188, 0.66291019345837021, array([ True,  True,  True, ..., False, False, False], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.182, 0.66183691539885692, array([ True,  True,  True, ..., False, False, False], dtype=bool))]), (500000, 0.2, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.363, 0.71574901392578283, array([ True,  True,  True, ..., False, False, False], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.374, 0.71530628672623364, array([ True,  True,  True, ..., False, False, False], dtype=bool))]), (500000, 0.4, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.663999, 0.72290643698516188, array([ True,  True,  True, ..., False, False, False], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.668, 0.72599211140626263, array([ True,  True,  True, ..., False, False, False], dtype=bool))]), (500000, 0.6, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 1.023995, 0.72640129866645198, array([ True,  True,  True, ..., False, False, False], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 1.032004, 0.72925219351203407, array([ True,  True,  True, ..., False, False, False], dtype=bool))]), (500000, 0.8, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 1.400002, 0.7320628404303845, array([ True,  True,  True, ..., False, False, False], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 1.367005, 0.72953392900265635, array([ True,  True,  True, ..., False, False, False], dtype=bool))]), (100000, 0.01, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.019003, 0.53695430518661624, array([ True,  True,  True, ..., False, False, False], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.018002, 0.53190319031903188, array([ True, False, False, ..., False, False, False], dtype=bool))]), (100000, 0.05, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.064993, 0.69847594515549116, array([ True,  True,  True, ..., False, False, False], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.063012, 0.69781185435616733, array([ True,  True,  True, ..., False, False, False], dtype=bool))]), (100000, 0.1, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.118004, 0.70472108186428395, array([ True,  True,  True, ..., False, False, False], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.118006, 0.70696825780138994, array([ True,  True,  True, ..., False, False, False], dtype=bool))]), (100000, 0.2, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.229002, 0.7159032976468378, array([ True,  True,  True, ..., False, False, False], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.235001, 0.71565510209557537, array([ True,  True,  True, ..., False, False, False], dtype=bool))]), (100000, 0.4, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.471995, 0.72204110654967935, array([ True,  True,  True, ..., False, False, False], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.480001, 0.72094099653867827, array([ True,  True,  True, ..., False, False, False], dtype=bool))]), (100000, 0.6, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.731993, 0.72430840645040118, array([ True,  True,  True, ..., False, False, False], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.726, 0.72447610614720004, array([ True,  True,  True, ..., False, False, False], dtype=bool))]), (100000, 0.8, [(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.991001, 0.72506640907993236, array([ True,  True,  True, ..., False, False, False], dtype=bool)), (MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.974999, 0.72443585821996836, array([ True,  True,  True, ..., False, False, False], dtype=bool))])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "results = []\n",
    "for reduceamount in [0, 2500000, 1000000, 500000, 100000]: #, 50000, 25000, 10000, 7500, 5000, 2500, 1500, 1000, 750, 500, 250, 100, 50, 10, 5]:\n",
    "    print(\"\\n\\t\\tReduction: \"+str(reduceamount))\n",
    "    for trainsize in [0.01, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8]:\n",
    "        print(\"\\n\\t\\tTraining size: \"+str(trainsize))\n",
    "        results.append((reduceamount,\n",
    "                       trainsize,\n",
    "                       ml.trainTest(X,\n",
    "                                    y,\n",
    "                                    classifiers=[MultinomialNB()],\n",
    "                                    reduce=reduceamount,\n",
    "                                    splits=2,\n",
    "                                    trainsize=trainsize,\n",
    "                                    testsize=0.2)))\n",
    "pickle.dump(results, open('pickled/-reddit-trained-mnbayes.pickle', 'wb'))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xyz = []\n",
    "for red, train, res in results:\n",
    "    acc = [r[2] for r in res]\n",
    "    xyz.append((red, train, sum(acc)/len(acc)))\n",
    "json_io.list_to_json(xyz, \"-reddit-reduction-trainsize-accuracy-mnbayes.json\", old_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tReduction: 0\n",
      "\n",
      "\t\tTraining size: 0.01\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 64\tScore:\t0.660871\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 65\tScore:\t0.662199\n",
      "\n",
      "\t\tTraining size: 0.05\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 187\tScore:\t0.706700\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 186\tScore:\t0.706814\n",
      "\n",
      "\t\tTraining size: 0.1\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 314\tScore:\t0.718493\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 314\tScore:\t0.721115\n",
      "\n",
      "\t\tTraining size: 0.2\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 1209\tScore:\t0.733203\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 1256\tScore:\t0.732888\n",
      "\n",
      "\t\tTraining size: 0.4\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 2596\tScore:\t0.749369\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 3403\tScore:\t0.745895\n",
      "\n",
      "\t\tTraining size: 0.6\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 2612\tScore:\t0.755266\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 4809\tScore:\t0.750805\n",
      "\n",
      "\t\tTraining size: 0.8\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\tTime: 2364\tScore:\t0.760840\n",
      "Starting to train <class 'sklearn.linear_model.logistic.LogisticRegression'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-af0fb0031a1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m                                     \u001b[0msplits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                                     \u001b[0mtrainsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainsize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                                     testsize=0.2)))\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pickled/-reddit-trained-log.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\CSC393 Sr Design\\src\\ml.py\u001b[0m in \u001b[0;36mtrainTest\u001b[1;34m(X, y, classifiers, reduce, splits, trainsize, testsize)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting to train %s\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m             \u001b[0mtraintime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1184\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m                 sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m         epsilon, sample_weight)\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m     \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "results = []\n",
    "for reduceamount in [0, 2500000, 1000000]: #, 50000, 25000, 10000, 7500, 5000, 2500, 1500, 1000, 750, 500, 250, 100, 50, 10, 5]:\n",
    "    print(\"\\n\\t\\tReduction: \"+str(reduceamount))\n",
    "    for trainsize in [0.8]:\n",
    "        print(\"\\n\\t\\tTraining size: \"+str(trainsize))\n",
    "        results.append((reduceamount,\n",
    "                       trainsize,\n",
    "                       ml.trainTest(X,\n",
    "                                    y,\n",
    "                                    classifiers=[LogisticRegression(n_jobs=-1)],\n",
    "                                    reduce=reduceamount,\n",
    "                                    splits=2,\n",
    "                                    trainsize=trainsize,\n",
    "                                    testsize=0.2)))\n",
    "pickle.dump(results, open('pickled/-reddit-trained-log.pickle', 'wb'))\n",
    "print(results)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xyz = []\n",
    "for red, train, res in results:\n",
    "    acc = [r[2] for r in res]\n",
    "    xyz.append((red, train, sum(acc)/len(acc)))\n",
    "json_io.list_to_json(xyz, \"-reddit-reduction-trainsize-accuracy-log.json\", old_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
