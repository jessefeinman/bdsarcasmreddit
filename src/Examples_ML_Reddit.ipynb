{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ml import *\n",
    "from json_io import *\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from dvs import DictVectorizerStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process tweets, if n is ommitted it processes all of them, sets lable and saves the processed tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = JSON_DIR+\"reddit/\"\n",
    "sarcastic_path = path+\"sarcastic/\"\n",
    "serious_path = path+\"serious/\"\n",
    "source = '-reddit-'\n",
    "features_path = 'features/'\n",
    "n=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File sarcastic0.json\ttime:\t0:15:16.696618\n",
      "Processed 100000 json lines\n",
      "File sarcastic100000.json\ttime:\t0:14:57.047648\n",
      "Processed 200000 json lines\n",
      "File sarcastic200000.json\ttime:\t0:14:41.658604\n",
      "Processed 300000 json lines\n",
      "File sarcastic300000.json\ttime:\t0:13:32.233240\n",
      "Processed 400000 json lines\n",
      "File sarcastic400000.json\ttime:\t0:13:15.141237\n",
      "Processed 500000 json lines\n",
      "File sarcastic500000.json\ttime:\t0:13:06.832235\n",
      "Processed 600000 json lines\n",
      "File sarcastic600000.json\ttime:\t0:13:07.965238\n",
      "Processed 700000 json lines\n",
      "File sarcastic700000.json\ttime:\t0:06:18.604113\n",
      "Processed 748124 json lines\n",
      "File serious0.json\ttime:\t0:13:34.035242\n",
      "Processed 100000 json lines\n",
      "File serious100000.json\ttime:\t0:13:17.878239\n",
      "Processed 200000 json lines\n",
      "File serious1000000.json\ttime:\t0:13:15.545238\n",
      "Processed 300000 json lines\n",
      "File serious1100000.json\ttime:\t0:13:19.632237\n",
      "Processed 400000 json lines\n",
      "File serious1200000.json\ttime:\t0:05:04.537094\n",
      "Processed 438060 json lines\n",
      "File serious200000.json\ttime:\t0:13:14.267238\n",
      "Processed 538060 json lines\n",
      "File serious300000.json\ttime:\t0:13:09.084235\n",
      "Processed 638060 json lines\n",
      "File serious400000.json\ttime:\t0:13:10.930236\n",
      "Processed 738060 json lines\n",
      "File serious500000.json\ttime:\t0:13:13.512236\n",
      "Processed 838060 json lines\n",
      "File serious600000.json\ttime:\t0:13:07.209235\n",
      "Processed 938060 json lines\n",
      "File serious700000.json\ttime:\t0:13:15.758239\n",
      "Processed 1038060 json lines\n",
      "File serious800000.json\ttime:\t0:13:13.986237\n",
      "Processed 1138060 json lines\n",
      "File serious900000.json\ttime:\t0:13:17.199238\n",
      "Processed 1238060 json lines\n"
     ]
    }
   ],
   "source": [
    "processRandomizeJson(sarcastic=True,\n",
    "                     json_path=sarcastic_path,\n",
    "                     features_path=features_path,\n",
    "                     source=source,\n",
    "                     n=n,\n",
    "                     cleanTokens=cleanTokensReddit)\n",
    "processRandomizeJson(sarcastic=False,\n",
    "                     json_path=serious_path,\n",
    "                     features_path=features_path,\n",
    "                     source=source,\n",
    "                     n=n,\n",
    "                     cleanTokens=cleanTokensReddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load random set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcasticFeats = loadProcessedFeatures(features_path,\n",
    "                                       source,\n",
    "                                       sarcastic=True,\n",
    "                                       n=n,\n",
    "                                       random=False)\n",
    "seriousFeats = loadProcessedFeatures(features_path,\n",
    "                                     source,\n",
    "                                     sarcastic=False,\n",
    "                                     n=n,\n",
    "                                     random=False)\n",
    "features = chain(sarcasticFeats, seriousFeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten feature dictionaries, if leaveout is a feature that feature is ommitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flattenDict(feature):\n",
    "    d = {}\n",
    "    for key, value in feature.items():\n",
    "        if isinstance(value, dict):\n",
    "            for subkey, subvalue in value.items():\n",
    "                d[subkey] = subvalue\n",
    "        else:\n",
    "            d[key] = value\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(features):\n",
    "    vocab = {}\n",
    "    for feats in features:\n",
    "        startTime = datetime.now()\n",
    "        for word in flattenDict(feat):\n",
    "            vocab.setdefault(word, len(vocab))\n",
    "    feature_names = sorted(vocab, key=vocab.get)\n",
    "    return vocab, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform(features, vocab={}):\n",
    "    dtype = np.float32\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    X = []\n",
    "    y = []\n",
    "    for feats, sarc in features:\n",
    "        startTime = datetime.now()\n",
    "        for word, val in flattenDict(feats).items():\n",
    "            if word in vocab:\n",
    "                indices.append(vocab[f])\n",
    "                X.append(dtype(v))\n",
    "        indptr.append(len(indices))\n",
    "        y.append(sarc)\n",
    "        if len(y) % 100000 == 0:\n",
    "            stopTime = datetime.now()\n",
    "            print(\"Time:\\t%s\" % (stopTime - startTime))\n",
    "            print(\"Processed %d vectors\"%len(y))\n",
    "            startTime = datetime.now()\n",
    "    return sp.csr_matrix((X, indices, indptr), dtype=dtype), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_transform(features, vocab={}):\n",
    "    dtype = np.float32\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    X = []\n",
    "    y = []\n",
    "    for feats, sarc in features:\n",
    "        startTime = datetime.now()\n",
    "        for word, val in flattenDict(feats).items():\n",
    "            if fit:\n",
    "                index = vocab.setdefault(word, len(vocab))\n",
    "                indices.append(index)\n",
    "                X.append(dtype(val))\n",
    "            elif word in vocab:\n",
    "                indices.append(vocab[f])\n",
    "                X.append(dtype(v))\n",
    "        indptr.append(len(indices))\n",
    "        y.append(sarc)\n",
    "        if len(y) % 100000 == 0:\n",
    "            stopTime = datetime.now()\n",
    "            print(\"Time:\\t%s\" % (stopTime - startTime))\n",
    "            print(\"Processed %d vectors\"%len(y))\n",
    "            startTime = datetime.now()\n",
    "    feature_names = sorted(vocab, key=vocab.get)\n",
    "    return sp.csr_matrix((X, indices, indptr), dtype=dtype), np.array(y), vocab, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:\t0:00:00\n",
      "Processed 100000 vectors\n",
      "Time:\t0:00:00\n",
      "Processed 200000 vectors\n",
      "Time:\t0:00:00.000979\n",
      "Processed 300000 vectors\n",
      "Time:\t0:00:00\n",
      "Processed 400000 vectors\n",
      "Time:\t0:00:00\n",
      "Processed 500000 vectors\n",
      "Time:\t0:00:00.001000\n",
      "Processed 600000 vectors\n",
      "Time:\t0:00:00.001000\n",
      "Processed 700000 vectors\n",
      "Time:\t0:00:00.001000\n",
      "Processed 800000 vectors\n",
      "Time:\t0:00:00\n",
      "Processed 900000 vectors\n",
      "Time:\t0:00:00\n",
      "Processed 1000000 vectors\n",
      "Time:\t0:00:00.001000\n",
      "Processed 1100000 vectors\n",
      "Time:\t0:00:00.001000\n",
      "Processed 1200000 vectors\n",
      "Time:\t0:00:00.001001\n",
      "Processed 1300000 vectors\n",
      "Time:\t0:00:00\n",
      "Processed 1400000 vectors\n",
      "Time:\t0:00:00.000999\n",
      "Processed 1500000 vectors\n",
      "Time:\t0:00:00\n",
      "Processed 1600000 vectors\n",
      "Time:\t0:00:00\n",
      "Processed 1700000 vectors\n",
      "Time:\t0:00:00\n",
      "Processed 1800000 vectors\n",
      "Time:\t0:00:00\n",
      "Processed 1900000 vectors\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-069e7617b31a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-c92f1e2b7931>\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(features, vocab)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mstartTime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sp' is not defined"
     ]
    }
   ],
   "source": [
    "(X,y,vocab, feature_names) = fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(vocab, open('pickled/-reddit-vectorizer.pickle', 'wb'))\n",
    "pickle.dump(y, open('pickled/-reddit-sarcasmVector.pickle', 'wb'))\n",
    "pickle.dump(X, open('pickled/-reddit-featureVector.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pickle.load(open('pickled/-reddit-featureVector.pickle', 'rb'))\n",
    "y = pickle.load(open('pickled/-reddit-sarcasmVector.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test, reports results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-120764d516b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m           \u001b[0msplits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m           \u001b[0mtrainsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m           testsize=0.2)\n\u001b[0m",
      "\u001b[1;32mC:\\dev\\CSC393 Sr Design\\src\\ml.py\u001b[0m in \u001b[0;36mtrainTest\u001b[1;34m(X, y, classifiers, reduce, splits, trainsize, testsize)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m             \u001b[0mtrainTime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,\n\u001b[1;32m-> 1173\u001b[1;33m                          order=\"C\")\n\u001b[0m\u001b[0;32m   1174\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    519\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    520\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[0;32m    522\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    380\u001b[0m                                       force_all_finite)\n\u001b[0;32m    381\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "trainTest(X,\n",
    "          y,\n",
    "          classifiers=DEFAULT_CLASSIFIERS,\n",
    "          reduce=0,\n",
    "          splits=1,\n",
    "          trainsize=0.2,\n",
    "          testsize=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Testing saved classifier with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:\t0.545430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.54542973180292997}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfClassifiersDV = loadClassifiersDV('pickled/2017-04-25205540579589 2voting.pickle')\n",
    "(classifier, dv) = listOfClassifiersDV[0]\n",
    "testSavedClassifier(features, sarcasm, classifier[0], dv)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
