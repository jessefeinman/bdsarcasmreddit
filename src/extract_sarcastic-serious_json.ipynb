{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import re\n",
    "from os import listdir, SEEK_END\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_re = [\n",
    "    r\"(\\/sarcasm)\",\n",
    "    r\"(\\[[!?]\\](?!\\())\",\n",
    "    r\"(:\\^(?! ?[D)(\\[\\]]))\",\n",
    "    r\"(<\\/?sarcasm>)\",\n",
    "    r\"(&lt;\\/?sarcasm&gt)\",\n",
    "    r\"(#sarcasm)\",\n",
    "    r\"( \\/s(?![a-zA-Z0-9]))\"\n",
    "          ]\n",
    "sarc = re.compile(r\"(\\.~)|(\\. ~)\")\n",
    "strike = re.compile(r\"(?P<strike>~~.*~~)\")\n",
    "sarcasm_re = re.compile('|'.join(list_re))\n",
    "\n",
    "filename_re = re.compile(r\"\"\"(RC_20\\d{2}-\\d{2}(?!\\.bz2))\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def commentGenerator():\n",
    "    for name in listdir():\n",
    "        if filename_re.match(name) is not None:\n",
    "            start = datetime.datetime.now()\n",
    "            print(\"Processing file %s, starttime: %s\"%(name, start))\n",
    "            pop = []\n",
    "            for n, line in enumerate(open(name)):\n",
    "            ## HERE to\n",
    "            ## ___________________________________________________________________________________________\n",
    "            ##\n",
    "                try:\n",
    "                    body = json.loads(line)['body']\n",
    "                    if 10 <= len(body) <= 300:\n",
    "                        compareText = body.lower()\n",
    "                        if(sarcasm_re.search(compareText) is not None) or \\\n",
    "                        ((sarc.search(compareText) is not None) and (strike.search(compareText) is None)):\n",
    "                            yield (True, body)\n",
    "                        else:\n",
    "                            pop.append(body)\n",
    "                            if len(pop) == 1000:\n",
    "                                yield (False, random.choice(pop))\n",
    "                                pop = []\n",
    "                except:\n",
    "                    print(\"Error on line %s, : %s\"%(n, line))\n",
    "            ## \n",
    "            ## _____________________________________________________________________________________________\n",
    "            ## HERE\n",
    "            ## The documents are in the correct format for spark to process with it's json library\n",
    "            ## There is at least 1 entry which is invalid so loading the body will result in an error\n",
    "            print(\"Finished: \" + str(datetime.datetime.now()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_rdd = sqlContext.read.format('json').load('json/serious100000.json')\n",
    "rdd = df_rdd.rdd\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def filterComments(generator):\n",
    "    import nlp\n",
    "    import ml\n",
    "    pop = []\n",
    "    for comment in generator:\n",
    "            text = comment['text'].lower()\n",
    "            if 10 <= len(text) <= 300:\n",
    "                if sarcasm_re.search(text) is not None or \\\n",
    "                (sarc.search(text) is not None and (strike.search(text) is not None)):\n",
    "                    yield (True, flattenDict(feature(comment['text'], nlp.ckeanTokensReddit)))\n",
    "                else:\n",
    "                    pop.append(comment['text'])\n",
    "                    if len(pop) == 1300:\n",
    "                        yield (False, flattenDict(feature(random.choice(pop), nlp.ckeanTokensReddit)))\n",
    "                        pop = []\n",
    "                        \n",
    "features = rdd.mapPartitions(filterComments)\n",
    "\n",
    "def getVocab(gen):\n",
    "    for sarc, features in gen:\n",
    "        for key in feature:\n",
    "            yield key\n",
    "\n",
    "from dvs import DictVectorizerPartial\n",
    "\n",
    "vocab = dict(features.mapPartitions(getVocab).unique().zipWithIndex().collect())\n",
    "\n",
    "dvp = DictVectorizerPartial(vocab=vocab) #this is not the correct syntax\n",
    "\n",
    "def vectorize(gen, dv):\n",
    "    blocksize = 100000\n",
    "    sarclst = []\n",
    "    featlst = []\n",
    "    for sarc, features in gen:\n",
    "        sarclst.append(sarc)\n",
    "        featlst.append(features)\n",
    "        if len(sarclst) == 100000:\n",
    "            yield (sarclst, dv.transform(featlst))\n",
    "            sarclst = []\n",
    "            featlst = []\n",
    "    yield (sarclst, dv.transform(featlst))\n",
    "\n",
    "vdvp = lambda gen: vectorize(gen, dvp)\n",
    "csrs = features.mapPartitions(vdvp)\n",
    "\n",
    "def vstack(a,b):\n",
    "    from scipy.sparse import csr_matrix, vstack\n",
    "    asarc = a[0]\n",
    "    acsr = a[1]\n",
    "    bsarc = b[0]\n",
    "    bcsr = b[1]\n",
    "    return asarc.append(bsarc), vstack([acsr,bcsr], format='csr')\n",
    "\n",
    "(y, X) = zip*(csrs.reduce(vstack).collect)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import ml\n",
    "\n",
    "results = ml.trainTest(X,\n",
    "                       y,\n",
    "                       classifiers=[LogisticRegression(n_jobs=-1)],\n",
    "                       reduce=0,\n",
    "                       splits=4,\n",
    "                       trainsize=0.8,\n",
    "                       testsize=0.2)\n",
    "pickle.dump(results, open('trained-logistic-classifier.pickle', 'wb'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sarcasticCount = 0\n",
    "seriousCount = 0\n",
    "sarcastic = open('json/sarcastic0.json', 'a')\n",
    "serious = open('json/serious0.json', 'a')\n",
    "sarcastic.write('[\\n')\n",
    "serious.write('[\\n')\n",
    "for n, (sarcasm, comment) in enumerate(commentGenerator()):\n",
    "    try:\n",
    "        if sarcasm:\n",
    "            json.dump({'text':comment}, sarcastic)\n",
    "            sarcasticCount += 1\n",
    "            if sarcasticCount % 100000 == 0:\n",
    "                serious.write('\\n]')\n",
    "                sarcastic.close()\n",
    "                sarcastic = open('json/sarcastic%d.json'%sarcasticCount, 'a')\n",
    "            else:\n",
    "                sarcastic.write(',\\n')\n",
    "        else:\n",
    "            json.dump({'text':comment}, serious)\n",
    "            seriousCount += 1\n",
    "            if seriousCount % 100000 == 0:\n",
    "                serious.write('\\n]')\n",
    "                serious.close()\n",
    "                serious = open('json/serious%d.json'%seriousCount, 'a')\n",
    "            else:\n",
    "                serious.write(',\\n')\n",
    "    except:\n",
    "        print(\"Error on index %s, : (%s, %s)\"%(n, sarcasm, comment))\n",
    "        sarcastic.close()\n",
    "        serious.close()\n",
    "        \n",
    "sarcastic.close()\n",
    "serious.close()\n",
    "\n",
    "sarcasticname = sarcastic.name\n",
    "seriousname = serious.name\n",
    "\n",
    "sarcastic.close()\n",
    "serious.close()\n",
    "\n",
    "sarcastic = open(sarcasticname, 'r+b')\n",
    "serious = open(seriousname, 'r+b')\n",
    "sarcastic.seek(-3, SEEK_END)\n",
    "sarcastic.write(bytes(\"\\r\\n]\", 'utf8'))\n",
    "serious.seek(-3, SEEK_END)\n",
    "serious.write(bytes(\"\\r\\n]\", 'utf8'))\n",
    "sarcastic.close()\n",
    "serious.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sarcastic.close()\n",
    "serious.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2.7]",
   "language": "python",
   "name": "conda-env-python2.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
